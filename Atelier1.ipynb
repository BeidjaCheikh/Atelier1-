{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751d6975",
   "metadata": {},
   "source": [
    "## Partie 1 : Pré-traitement de texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f997b2",
   "metadata": {},
   "source": [
    "### 1. Créer un corpus qui contient les textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff93d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ecb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Étape 1 : Créer un corpus de texte\n",
    "corpus = [\n",
    "    \"Le chat dort sur le tapis.\",\n",
    "    \"Les Oiseaux Chantent Le Matin.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Mangeons des pommes délicieuses.\",\n",
    "    \"Je mange une orange fraîche.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9516a",
   "metadata": {},
   "source": [
    "## 2. Convertir le corpus en type DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d318b2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte\n",
      "0        Le chat dort sur le tapis.\n",
      "1    Les Oiseaux Chantent Le Matin.\n",
      "2    Le chien court dans le jardin.\n",
      "3  Mangeons des pommes délicieuses.\n",
      "4      Je mange une orange fraîche.\n"
     ]
    }
   ],
   "source": [
    "# Étape 2 : Convertir le corpus en DataFrame\n",
    "data = {'texte': corpus}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43187547",
   "metadata": {},
   "source": [
    "### 3. Quelles sont les différentes approches pour prétraiter un corpus de textes? Décrire les."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff450d4d",
   "metadata": {},
   "source": [
    "\n",
    "1. **Tokenisation** :\n",
    "   - **Définition** : La tokenisation consiste à diviser le texte en unités plus petites, appelées \"tokens\". Ces tokens peuvent être des mots, des phrases, ou même des caractères, selon le niveau de granularité souhaité.\n",
    "   - **Exemple** : La phrase \"Le chat dort sur le tapis.\" serait tokenisée en [\"Le\", \"chat\", \"dort\", \"sur\", \"le\", \"tapis\", \".\"].\n",
    "\n",
    "2. **Minuscules** (Lowercasing) :\n",
    "   - **Définition** : Convertir tous les caractères du texte en minuscules. Cela aide à éviter que le modèle ne considère les mots en majuscules et en minuscules comme différents.\n",
    "   - **Exemple** : \"Chat\" devient \"chat\".\n",
    "\n",
    "3. **Suppression de la ponctuation** :\n",
    "   - **Définition** : Enlever les signes de ponctuation du texte. Cela peut être utile lorsque la ponctuation n'apporte pas d'information significative pour la tâche en question.\n",
    "   - **Exemple** : \"Mangeons des pommes délicieuses.\" devient \"Mangeons des pommes délicieuses\"\n",
    "\n",
    "4. **Stop Words** :\n",
    "   - **Définition** : Supprimer les mots courants (comme \"le\", \"la\", \"de\", \"et\", etc.) qui n'apportent souvent pas beaucoup d'information pour la tâche en cours.\n",
    "   - **Exemple** : \"Le chat dort sur le tapis.\" devient \"chat dort tapis\".\n",
    "\n",
    "5. **Stemming et Lemmatisation** :\n",
    "   - **Stemming** : Réduire les mots à leur forme de base en supprimant les suffixes. Par exemple, \"jouer\", \"jouant\", \"jouerai\", deviennent tous \"jou\".\n",
    "   - **Lemmatisation** : Réduire les mots à leur forme canonique (lemme) en utilisant des dictionnaires linguistiques pour regrouper les mots de différentes formes.\n",
    "   \n",
    "6. **Encodage** :\n",
    "   - **Définition** : Convertir les mots en représentations numériques (par exemple, en utilisant un encodage comme One-Hot ou Word Embeddings) pour que les modèles de machine learning puissent les traiter.\n",
    "  \n",
    "7. **Suppression de caractères spéciaux et numéros** :\n",
    "   - **Définition** : Si les caractères spéciaux ou les chiffres ne sont pas pertinents pour la tâche, il peut être utile de les supprimer.\n",
    "\n",
    "8. **Nettoyage de texte spécifique à la tâche** :\n",
    "   - Selon la tâche à accomplir, d'autres étapes de prétraitement spécifiques peuvent être nécessaires. Par exemple, pour la classification d'emails, il peut être important de supprimer les adresses email et les noms de domaine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de45daf",
   "metadata": {},
   "source": [
    "### 4. Décrire le code suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dad78bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "# Imprimer tous les caractères de ponctuation\n",
    "print(string.punctuation)\n",
    "\n",
    "# Définir une fonction 'code' qui filtre les caractères de ponctuation d'un texte\n",
    "def code(texte):\n",
    "    text_s_pon = [c for c in texte if c not in string.punctuation]\n",
    "    return ''.join(text_s_pon)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24f5ff55",
   "metadata": {},
   "source": [
    "ce code définit une fonction code qui prend un texte en entrée, filtre les caractères de ponctuation et renvoie le texte sans ces caractères"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91949b2c",
   "metadata": {},
   "source": [
    " ### 5. Ajouter une colonne dans l’objet corpus nommée « t_s_p » en utilisant la fonction la fonctionde Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dbc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis\n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin\n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin\n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses\n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction 'code' pour enlever la ponctuation\n",
    "df['t_s_p'] = df['texte'].apply(code)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f95767",
   "metadata": {},
   "source": [
    "### 6. Ecrire une fonction pour tokinezer le corpus de colonne « t_s_p »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d09bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8221f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens  \n",
      "0      [Le, chat, dort, sur, le, tapis]  \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]  \n",
      "2  [Le, chien, court, dans, le, jardin]  \n",
      "3  [Mangeons, des, pommes, délicieuses]  \n",
      "4     [Je, mange, une, orange, fraîche]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires pour la tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Appliquer la fonction 'tokenize' à la colonne 't_s_p'\n",
    "df['tokens'] = df['t_s_p'].apply(tokenize)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1113e",
   "metadata": {},
   "source": [
    "### 7. Ecrire une fonction qui élimine les stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb8acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assurez-vous d'avoir installé la bibliothèque nltk et téléchargé les ressources nécessaires\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118f5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire la fonction pour éliminer les stop words :\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Récupérer la liste des mots vides pour le français\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    \n",
    "    # Filtrer les tokens pour enlever les mots vides\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af33715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                   tokens_cleaned  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                 lemmatized_tokens              lemmatized_text  \\\n",
      "0              [chat, dort, tapis]              chat dort tapis   \n",
      "1       [Oiseaux, Chantent, Matin]       Oiseaux Chantent Matin   \n",
      "2           [chien, court, jardin]           chien court jardin   \n",
      "3  [Mangeons, pommes, délicieuses]  Mangeons pommes délicieuses   \n",
      "4         [mange, orange, fraîche]         mange orange fraîche   \n",
      "\n",
      "  lemmatized_and_stemmed_tokens lemmatized_and_stemmed_text  \n",
      "0            [chat, dort, tapi]              chat dort tapi  \n",
      "1    [oiseaux, chantent, matin]      oiseaux chantent matin  \n",
      "2        [chien, court, jardin]          chien court jardin  \n",
      "3    [mangeon, pomm, délicieus]      mangeon pomm délicieus  \n",
      "4         [mang, orang, fraîch]           mang orang fraîch  \n"
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction remove_stopwords à la colonne 'tokens'\n",
    "df['tokens_cleaned'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b235",
   "metadata": {},
   "source": [
    "### 8. Appliquer la lemmatisation et stremming sur le corpus sans stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0948914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# def lemmatize_and_stem(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     stemmer = PorterStemmer()\n",
    "    \n",
    "#     tokens = word_tokenize(text)\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "#     stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "    \n",
    "#     return ' '.join(stemmed_tokens)\n",
    "\n",
    "# # Supposons que 't_s_p' est déjà ajoutée à votre DataFrame df\n",
    "# df['lemmatized_and_stemmed_text'] = df['tokens'].apply(lemmatize_and_stem)\n",
    "\n",
    "# # Afficher le DataFrame mis à jour\n",
    "# print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cd97ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                   tokens_cleaned  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                 lemmatized_tokens              lemmatized_text  \\\n",
      "0              [chat, dort, tapis]              chat dort tapis   \n",
      "1       [Oiseaux, Chantent, Matin]       Oiseaux Chantent Matin   \n",
      "2           [chien, court, jardin]           chien court jardin   \n",
      "3  [Mangeons, pommes, délicieuses]  Mangeons pommes délicieuses   \n",
      "4         [mange, orange, fraîche]         mange orange fraîche   \n",
      "\n",
      "  lemmatized_and_stemmed_tokens lemmatized_and_stemmed_text  \n",
      "0            [chat, dort, tapi]              chat dort tapi  \n",
      "1    [oiseaux, chantent, matin]      oiseaux chantent matin  \n",
      "2        [chien, court, jardin]          chien court jardin  \n",
      "3    [mangeon, pomm, délicieus]      mangeon pomm délicieus  \n",
      "4         [mang, orang, fraîch]           mang orang fraîch  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importer la bibliothèque nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Définir la fonction de lemmatisation\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Appliquer la lemmatisation à la colonne 'tokens_cleaned'\n",
    "df['lemmatized_tokens'] = df['tokens_cleaned'].apply(lemmatize_tokens)\n",
    "\n",
    "# Convertir les listes de tokens en chaînes de caractères\n",
    "df['lemmatized_text'] = df['lemmatized_tokens'].apply(' '.join)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441d72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cab3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                   tokens_cleaned  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                 lemmatized_tokens              lemmatized_text  \\\n",
      "0              [chat, dort, tapis]              chat dort tapis   \n",
      "1       [Oiseaux, Chantent, Matin]       Oiseaux Chantent Matin   \n",
      "2           [chien, court, jardin]           chien court jardin   \n",
      "3  [Mangeons, pommes, délicieuses]  Mangeons pommes délicieuses   \n",
      "4         [mange, orange, fraîche]         mange orange fraîche   \n",
      "\n",
      "  lemmatized_and_stemmed_tokens lemmatized_and_stemmed_text  \n",
      "0            [chat, dort, tapi]              chat dort tapi  \n",
      "1    [oiseaux, chantent, matin]      oiseaux chantent matin  \n",
      "2        [chien, court, jardin]          chien court jardin  \n",
      "3    [mangeon, pomm, délicieus]      mangeon pomm délicieus  \n",
      "4         [mang, orang, fraîch]           mang orang fraîch  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importer la bibliothèque nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Définir la fonction de stemming\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Appliquer le stemming à la colonne 'tokens_cleaned'\n",
    "df['lemmatized_and_stemmed_tokens'] = df['lemmatized_tokens'].apply(stem_tokens)\n",
    "# Convertir les listes de tokens en chaînes de caractères\n",
    "df['lemmatized_and_stemmed_text'] = df['lemmatized_and_stemmed_tokens'].apply(' '.join)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a3981",
   "metadata": {},
   "source": [
    "## Partie 2 : CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb22ab7",
   "metadata": {},
   "source": [
    "### 9. Initialiser et ajuster le CountVectorizer à votre corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07385b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a74318f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chantent' 'chat' 'chien' 'court' 'dort' 'délicieus' 'fraîch' 'jardin'\n",
      " 'mang' 'mangeon' 'matin' 'oiseaux' 'orang' 'pomm' 'tapi']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialiser le CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Ajuster le vectorizer à votre corpus\n",
    "X = vectorizer.fit_transform(df['lemmatized_and_stemmed_text'])\n",
    "\n",
    "# Afficher les mots du vocabulaire\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15a96c",
   "metadata": {},
   "source": [
    "### 10. Transformer le corpus en une matrice de comptage de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c045750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser le vectorizer précédemment initialisé et ajusté\n",
    "X = vectorizer.transform(df['lemmatized_and_stemmed_text'])\n",
    "\n",
    "# Convertir la matrice en un DataFrame pour une meilleure visualisation\n",
    "import pandas as pd\n",
    "count_matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c350ce",
   "metadata": {},
   "source": [
    "### 11. Explorer la matrice résultante pour comprendre comment les tokens sont représentés en vecteurs binaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6188bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chantent  chat  chien  court  dort  délicieus  fraîch  jardin  mang  \\\n",
      "0         0     1      0      0     1          0       0       0     0   \n",
      "1         1     0      0      0     0          0       0       0     0   \n",
      "2         0     0      1      1     0          0       0       1     0   \n",
      "3         0     0      0      0     0          1       0       0     0   \n",
      "4         0     0      0      0     0          0       1       0     1   \n",
      "\n",
      "   mangeon  matin  oiseaux  orang  pomm  tapi  \n",
      "0        0      0        0      0     0     1  \n",
      "1        0      1        1      0     0     0  \n",
      "2        0      0        0      0     0     0  \n",
      "3        1      0        0      0     1     0  \n",
      "4        0      0        0      1     0     0  \n"
     ]
    }
   ],
   "source": [
    "# Afficher la matrice de comptage de tokens\n",
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be9408",
   "metadata": {},
   "source": [
    "## Partie 3 : TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6becbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialiser le TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajuster le TfidfVectorizer à votre corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_and_stemmed_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633ad4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chantent     chat    chien    court     dort  délicieus   fraîch   jardin  \\\n",
      "0   0.00000  0.57735  0.00000  0.00000  0.57735    0.00000  0.00000  0.00000   \n",
      "1   0.57735  0.00000  0.00000  0.00000  0.00000    0.00000  0.00000  0.00000   \n",
      "2   0.00000  0.00000  0.57735  0.57735  0.00000    0.00000  0.00000  0.57735   \n",
      "3   0.00000  0.00000  0.00000  0.00000  0.00000    0.57735  0.00000  0.00000   \n",
      "4   0.00000  0.00000  0.00000  0.00000  0.00000    0.00000  0.57735  0.00000   \n",
      "\n",
      "      mang  mangeon    matin  oiseaux    orang     pomm     tapi  \n",
      "0  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.57735  \n",
      "1  0.00000  0.00000  0.57735  0.57735  0.00000  0.00000  0.00000  \n",
      "2  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
      "3  0.00000  0.57735  0.00000  0.00000  0.00000  0.57735  0.00000  \n",
      "4  0.57735  0.00000  0.00000  0.00000  0.57735  0.00000  0.00000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir la matrice TF-IDF en un DataFrame pour une meilleure visualisation\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f97654",
   "metadata": {},
   "source": [
    " ### 3. Explorer la matrice résultante pour comprendre comment les tokens sont représentés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a385a954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids TF-IDF pour le terme 'chat':\n",
      "0    0.57735\n",
      "1    0.00000\n",
      "2    0.00000\n",
      "3    0.00000\n",
      "4    0.00000\n",
      "Name: chat, dtype: float64\n",
      "\n",
      "Poids TF-IDF pour le terme 'chien':\n",
      "0    0.00000\n",
      "1    0.00000\n",
      "2    0.57735\n",
      "3    0.00000\n",
      "4    0.00000\n",
      "Name: chien, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Afficher les poids TF-IDF pour les termes \"chat\" et \"chien\"\n",
    "print(\"Poids TF-IDF pour le terme 'chat':\")\n",
    "print(tfidf_df['chat'])\n",
    "\n",
    "print(\"\\nPoids TF-IDF pour le terme 'chien':\")\n",
    "print(tfidf_df['chien'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b6831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9995f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité de cosinus entre 'chat' et 'chien': 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sélectionner les vecteurs TF-IDF pour les termes \"chat\" et \"chien\"\n",
    "vector_chat = tfidf_df[['chat']].values\n",
    "vector_chien = tfidf_df[['chien']].values\n",
    "\n",
    "# Calculer la similarité de cosinus\n",
    "similarity_chat_chien = cosine_similarity(vector_chat, vector_chien)\n",
    "\n",
    "# Afficher la similarité\n",
    "print(f\"Similarité de cosinus entre 'chat' et 'chien': {similarity_chat_chien[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb3d17",
   "metadata": {},
   "source": [
    "# Partie 4 : Application sur dataset YourubeSpam collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224dec8",
   "metadata": {},
   "source": [
    "### https://archive.ics.uci.edu/dataset/380/youtube+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4907dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fd7afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Chargez les données pour chaque artiste\n",
    "df_psy = pd.read_csv('Youtube01-Psy.csv')\n",
    "df_katy = pd.read_csv('Youtube02-KatyPerry.csv')\n",
    "df_lmfao = pd.read_csv('Youtube03-LMFAO.csv')\n",
    "df_eminem = pd.read_csv('Youtube04-Eminem.csv')\n",
    "df_shakira = pd.read_csv('Youtube05-Shakira.csv')\n",
    "\n",
    "# Concaténez les DataFrames\n",
    "df = pd.concat([df_psy, df_katy, df_lmfao, df_eminem, df_shakira], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdd7af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMMENT_ID</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>DATE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>CLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU</td>\n",
       "      <td>Julius NM</td>\n",
       "      <td>2013-11-07T06:20:48</td>\n",
       "      <td>Huh, anyway check out this you[tube] channel: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A</td>\n",
       "      <td>adam riyati</td>\n",
       "      <td>2013-11-07T12:37:15</td>\n",
       "      <td>Hey guys check out my new channel and our firs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8</td>\n",
       "      <td>Evgeny Murashkin</td>\n",
       "      <td>2013-11-08T17:34:21</td>\n",
       "      <td>just for test I have to say murdev.com</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>z13jhp0bxqncu512g22wvzkasxmvvzjaz04</td>\n",
       "      <td>ElNino Melendez</td>\n",
       "      <td>2013-11-09T08:28:43</td>\n",
       "      <td>me shaking my sexy ass on my channel enjoy ^_^ ﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z13fwbwp1oujthgqj04chlngpvzmtt3r3dw</td>\n",
       "      <td>GsMega</td>\n",
       "      <td>2013-11-10T16:05:38</td>\n",
       "      <td>watch?v=vtaRGgvGtWQ   Check this out .﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>_2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA</td>\n",
       "      <td>Katie Mettam</td>\n",
       "      <td>2013-07-13T13:27:39.441000</td>\n",
       "      <td>I love this song because we sing it at Camp al...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>_2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI</td>\n",
       "      <td>Sabina Pearson-Smith</td>\n",
       "      <td>2013-07-13T13:14:30.021000</td>\n",
       "      <td>I love this song for two reasons: 1.it is abou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>_2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs</td>\n",
       "      <td>jeffrey jules</td>\n",
       "      <td>2013-07-13T12:09:31.188000</td>\n",
       "      <td>wow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>_2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0</td>\n",
       "      <td>Aishlin Maciel</td>\n",
       "      <td>2013-07-13T11:17:52.308000</td>\n",
       "      <td>Shakira u are so wiredo</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>_2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA</td>\n",
       "      <td>Latin Bosch</td>\n",
       "      <td>2013-07-12T22:33:27.916000</td>\n",
       "      <td>Shakira is the best dancer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1956 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       COMMENT_ID                AUTHOR  \\\n",
       "0     LZQPQhLyRh80UYxNuaDWhIGQYNQ96IuCg-AYWqNPjpU             Julius NM   \n",
       "1     LZQPQhLyRh_C2cTtd9MvFRJedxydaVW-2sNg5Diuo4A           adam riyati   \n",
       "2     LZQPQhLyRh9MSZYnf8djyk0gEF9BHDPYrrK-qCczIY8      Evgeny Murashkin   \n",
       "3             z13jhp0bxqncu512g22wvzkasxmvvzjaz04       ElNino Melendez   \n",
       "4             z13fwbwp1oujthgqj04chlngpvzmtt3r3dw                GsMega   \n",
       "...                                           ...                   ...   \n",
       "1951  _2viQ_Qnc6-bMSjqyL1NKj57ROicCSJV5SwTrw-RFFA          Katie Mettam   \n",
       "1952  _2viQ_Qnc6-pY-1yR6K2FhmC5i48-WuNx5CumlHLDAI  Sabina Pearson-Smith   \n",
       "1953  _2viQ_Qnc6_k_n_Bse9zVhJP8tJReZpo8uM2uZfnzDs         jeffrey jules   \n",
       "1954  _2viQ_Qnc6_yBt8UGMWyg3vh0PulTqcqyQtdE7d4Fl0        Aishlin Maciel   \n",
       "1955  _2viQ_Qnc685RPw1aSa1tfrIuHXRvAQ2rPT9R06KTqA           Latin Bosch   \n",
       "\n",
       "                            DATE  \\\n",
       "0            2013-11-07T06:20:48   \n",
       "1            2013-11-07T12:37:15   \n",
       "2            2013-11-08T17:34:21   \n",
       "3            2013-11-09T08:28:43   \n",
       "4            2013-11-10T16:05:38   \n",
       "...                          ...   \n",
       "1951  2013-07-13T13:27:39.441000   \n",
       "1952  2013-07-13T13:14:30.021000   \n",
       "1953  2013-07-13T12:09:31.188000   \n",
       "1954  2013-07-13T11:17:52.308000   \n",
       "1955  2013-07-12T22:33:27.916000   \n",
       "\n",
       "                                                CONTENT  CLASS  \n",
       "0     Huh, anyway check out this you[tube] channel: ...      1  \n",
       "1     Hey guys check out my new channel and our firs...      1  \n",
       "2                just for test I have to say murdev.com      1  \n",
       "3      me shaking my sexy ass on my channel enjoy ^_^ ﻿      1  \n",
       "4               watch?v=vtaRGgvGtWQ   Check this out .﻿      1  \n",
       "...                                                 ...    ...  \n",
       "1951  I love this song because we sing it at Camp al...      0  \n",
       "1952  I love this song for two reasons: 1.it is abou...      0  \n",
       "1953                                                wow      0  \n",
       "1954                            Shakira u are so wiredo      0  \n",
       "1955                         Shakira is the best dancer      0  \n",
       "\n",
       "[1956 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb26b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2400f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage du texte\n",
    "def clean_text(text):\n",
    "    # Supprimer les caractères spéciaux et les chiffres\n",
    "    text = ''.join([char for char in text if char.isalpha() or char.isspace()])\n",
    "    # Convertir en minuscules\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "df['cleaned_content'] = df['CONTENT'].apply(clean_text)\n",
    "\n",
    "# Tokenization\n",
    "df['tokens'] = df['cleaned_content'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03917879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       huh anyway check out this youtube channel koby...\n",
       "1       hey guys check out my new channel and our firs...\n",
       "2                   just for test i have to say murdevcom\n",
       "3            me shaking my sexy ass on my channel enjoy  \n",
       "4                     watchvvtarggvgtwq   check this out \n",
       "                              ...                        \n",
       "1951    i love this song because we sing it at camp al...\n",
       "1952    i love this song for two reasons it is about a...\n",
       "1953                                                  wow\n",
       "1954                              shakira u are so wiredo\n",
       "1955                           shakira is the best dancer\n",
       "Name: cleaned_content, Length: 1956, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7632ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc94b567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [huh, anyway, check, youtube, channel, kobyoshi]\n",
       "1       [hey, guys, check, new, channel, first, vid, u...\n",
       "2                                  [test, say, murdevcom]\n",
       "3                    [shaking, sexy, ass, channel, enjoy]\n",
       "4                              [watchvvtarggvgtwq, check]\n",
       "                              ...                        \n",
       "1951                       [love, song, sing, camp, time]\n",
       "1952    [love, song, two, reasons, africa, born, beaut...\n",
       "1953                                                [wow]\n",
       "1954                                 [shakira, u, wiredo]\n",
       "1955                              [shakira, best, dancer]\n",
       "Name: tokens, Length: 1956, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c877ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation et Stemming\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize_and_stem(tokens):\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "df['stemmed_tokens'] = df['tokens'].apply(lemmatize_and_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bfd9a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [huh, anyway, check, youtub, channel, kobyoshi]\n",
       "1       [hey, guy, check, new, channel, first, vid, u,...\n",
       "2                                  [test, say, murdevcom]\n",
       "3                       [shake, sexi, as, channel, enjoy]\n",
       "4                              [watchvvtarggvgtwq, check]\n",
       "                              ...                        \n",
       "1951                       [love, song, sing, camp, time]\n",
       "1952    [love, song, two, reason, africa, born, beauti...\n",
       "1953                                                [wow]\n",
       "1954                                 [shakira, u, wiredo]\n",
       "1955                              [shakira, best, dancer]\n",
       "Name: stemmed_tokens, Length: 1956, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stemmed_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bac3162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les listes de tokens en texte\n",
    "df['stemmed_text'] = df['stemmed_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Vectorisation avec TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['stemmed_text'])\n",
    "\n",
    "# Maintenant, tfidf_matrix contient la représentation vectorielle de votre texte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c920fbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aaaaaaa  abbastfuck  abl  ablaz  abomin  abonn  absolut  absorb  abus  \\\n",
      "0         0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "1         0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "2         0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "3         0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "4         0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "...       ...         ...  ...    ...     ...    ...      ...     ...   ...   \n",
      "1951      0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "1952      0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "1953      0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "1954      0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "1955      0.0         0.0  0.0    0.0     0.0    0.0      0.0     0.0   0.0   \n",
      "\n",
      "      abusedmistr  ...   οh   لل   ஜஜ  강남스타일  ｃｏｍｍｅｎｔ  ｄａｍｎ  ｆａｎｃi  \\\n",
      "0             0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "1             0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "2             0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "3             0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "4             0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "...           ...  ...  ...  ...  ...    ...      ...   ...    ...   \n",
      "1951          0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "1952          0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "1953          0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "1954          0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "1955          0.0  ...  0.0  0.0  0.0    0.0      0.0   0.0    0.0   \n",
      "\n",
      "      ｈｔｔｐｗｗｗｅｂａｙｃｏｍｕｓｒｓｈｏｅｃｏｌｌｅｃｔｏｒ   ｉｓ  ｔｈｉｓ  \n",
      "0                                0.0  0.0   0.0  \n",
      "1                                0.0  0.0   0.0  \n",
      "2                                0.0  0.0   0.0  \n",
      "3                                0.0  0.0   0.0  \n",
      "4                                0.0  0.0   0.0  \n",
      "...                              ...  ...   ...  \n",
      "1951                             0.0  0.0   0.0  \n",
      "1952                             0.0  0.0   0.0  \n",
      "1953                             0.0  0.0   0.0  \n",
      "1954                             0.0  0.0   0.0  \n",
      "1955                             0.0  0.0   0.0  \n",
      "\n",
      "[1956 rows x 3370 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir la matrice TF-IDF en un DataFrame pour une meilleure visualisation\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7dec2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 1 1\n",
      " 1 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0\n",
      " 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 1 0\n",
      " 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 0 1 0 0 1 0 0 1 1 1 0 1 0 1 0\n",
      " 0 1 1 1 0 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1\n",
      " 1 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1\n",
      " 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 0\n",
      " 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1\n",
      " 0 1 1 1 1 0 0 0 1 1 1 1 0 0 1 0 0 0 0 0 0 0]\n",
      "================================================================================\n",
      "Accuracy : 0.8724489795918368\n",
      "================================================================================\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.94      0.87       176\n",
      "           1       0.94      0.82      0.88       216\n",
      "\n",
      "    accuracy                           0.87       392\n",
      "   macro avg       0.88      0.88      0.87       392\n",
      "weighted avg       0.88      0.87      0.87       392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['CLASS'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser le modèle\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Entraîner le modèle\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "# Évaluer la performance du modèle\n",
    "print(\"=\"*80)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy : {accuracy}\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Classification Report :\\n{report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04b0787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f25f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06e619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdd324f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
