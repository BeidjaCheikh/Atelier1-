{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751d6975",
   "metadata": {},
   "source": [
    "## Partie 1 : Pré-traitement de texte\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f997b2",
   "metadata": {},
   "source": [
    "### 1. Créer un corpus qui contient les textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff93d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ecb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Étape 1 : Créer un corpus de texte\n",
    "corpus = [\n",
    "    \"Le chat dort sur le tapis.\",\n",
    "    \"Les Oiseaux Chantent Le Matin.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Mangeons des pommes délicieuses.\",\n",
    "    \"Je mange une orange fraîche.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9516a",
   "metadata": {},
   "source": [
    "## 2. Convertir le corpus en type DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d318b2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte\n",
      "0        Le chat dort sur le tapis.\n",
      "1    Les Oiseaux Chantent Le Matin.\n",
      "2    Le chien court dans le jardin.\n",
      "3  Mangeons des pommes délicieuses.\n",
      "4      Je mange une orange fraîche.\n"
     ]
    }
   ],
   "source": [
    "# Étape 2 : Convertir le corpus en DataFrame\n",
    "data = {'texte': corpus}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43187547",
   "metadata": {},
   "source": [
    "### 3. Quelles sont les différentes approches pour prétraiter un corpus de textes? Décrire les."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff450d4d",
   "metadata": {},
   "source": [
    "\n",
    "1. **Tokenisation** :\n",
    "   - **Définition** : La tokenisation consiste à diviser le texte en unités plus petites, appelées \"tokens\". Ces tokens peuvent être des mots, des phrases, ou même des caractères, selon le niveau de granularité souhaité.\n",
    "   - **Exemple** : La phrase \"Le chat dort sur le tapis.\" serait tokenisée en [\"Le\", \"chat\", \"dort\", \"sur\", \"le\", \"tapis\", \".\"].\n",
    "\n",
    "2. **Minuscules** (Lowercasing) :\n",
    "   - **Définition** : Convertir tous les caractères du texte en minuscules. Cela aide à éviter que le modèle ne considère les mots en majuscules et en minuscules comme différents.\n",
    "   - **Exemple** : \"Chat\" devient \"chat\".\n",
    "\n",
    "3. **Suppression de la ponctuation** :\n",
    "   - **Définition** : Enlever les signes de ponctuation du texte. Cela peut être utile lorsque la ponctuation n'apporte pas d'information significative pour la tâche en question.\n",
    "   - **Exemple** : \"Mangeons des pommes délicieuses.\" devient \"Mangeons des pommes délicieuses\"\n",
    "\n",
    "4. **Stop Words** :\n",
    "   - **Définition** : Supprimer les mots courants (comme \"le\", \"la\", \"de\", \"et\", etc.) qui n'apportent souvent pas beaucoup d'information pour la tâche en cours.\n",
    "   - **Exemple** : \"Le chat dort sur le tapis.\" devient \"chat dort tapis\".\n",
    "\n",
    "5. **Stemming et Lemmatisation** :\n",
    "   - **Stemming** : Réduire les mots à leur forme de base en supprimant les suffixes. Par exemple, \"jouer\", \"jouant\", \"jouerai\", deviennent tous \"jou\".\n",
    "   - **Lemmatisation** : Réduire les mots à leur forme canonique (lemme) en utilisant des dictionnaires linguistiques pour regrouper les mots de différentes formes.\n",
    "   \n",
    "6. **Encodage** :\n",
    "   - **Définition** : Convertir les mots en représentations numériques (par exemple, en utilisant un encodage comme One-Hot ou Word Embeddings) pour que les modèles de machine learning puissent les traiter.\n",
    "  \n",
    "7. **Suppression de caractères spéciaux et numéros** :\n",
    "   - **Définition** : Si les caractères spéciaux ou les chiffres ne sont pas pertinents pour la tâche, il peut être utile de les supprimer.\n",
    "\n",
    "8. **Nettoyage de texte spécifique à la tâche** :\n",
    "   - Selon la tâche à accomplir, d'autres étapes de prétraitement spécifiques peuvent être nécessaires. Par exemple, pour la classification d'emails, il peut être important de supprimer les adresses email et les noms de domaine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de45daf",
   "metadata": {},
   "source": [
    "### 4. Décrire le code suivant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dad78bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "\n",
    "# Imprimer tous les caractères de ponctuation\n",
    "print(string.punctuation)\n",
    "\n",
    "# Définir une fonction 'code' qui filtre les caractères de ponctuation d'un texte\n",
    "def code(texte):\n",
    "    text_s_pon = [c for c in texte if c not in string.punctuation]\n",
    "    return ''.join(text_s_pon)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24f5ff55",
   "metadata": {},
   "source": [
    "ce code définit une fonction code qui prend un texte en entrée, filtre les caractères de ponctuation et renvoie le texte sans ces caractères"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91949b2c",
   "metadata": {},
   "source": [
    " ### 5. Ajouter une colonne dans l’objet corpus nommée « t_s_p » en utilisant la fonction la fonctionde Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dbc2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis\n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin\n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin\n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses\n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche\n"
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction 'code' pour enlever la ponctuation\n",
    "df['t_s_p'] = df['texte'].apply(code)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f95767",
   "metadata": {},
   "source": [
    "### 6. Ecrire une fonction pour tokinezer le corpus de colonne « t_s_p »."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d09bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b8221f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens  \n",
      "0      [Le, chat, dort, sur, le, tapis]  \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]  \n",
      "2  [Le, chien, court, dans, le, jardin]  \n",
      "3  [Mangeons, des, pommes, délicieuses]  \n",
      "4     [Je, mange, une, orange, fraîche]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Télécharger les ressources nécessaires pour la tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Appliquer la fonction 'tokenize' à la colonne 't_s_p'\n",
    "df['tokens'] = df['t_s_p'].apply(tokenize)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf1113e",
   "metadata": {},
   "source": [
    "### 7. Ecrire une fonction qui élimine les stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb8acac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assurez-vous d'avoir installé la bibliothèque nltk et téléchargé les ressources nécessaires\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118f5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire la fonction pour éliminer les stop words :\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Récupérer la liste des mots vides pour le français\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    \n",
    "    # Filtrer les tokens pour enlever les mots vides\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af33715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                   tokens_cleaned  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                 lemmatized_tokens              lemmatized_text  \\\n",
      "0              [chat, dort, tapis]              chat dort tapis   \n",
      "1       [Oiseaux, Chantent, Matin]       Oiseaux Chantent Matin   \n",
      "2           [chien, court, jardin]           chien court jardin   \n",
      "3  [Mangeons, pommes, délicieuses]  Mangeons pommes délicieuses   \n",
      "4         [mange, orange, fraîche]         mange orange fraîche   \n",
      "\n",
      "  lemmatized_and_stemmed_tokens lemmatized_and_stemmed_text  \n",
      "0            [chat, dort, tapi]              chat dort tapi  \n",
      "1    [oiseaux, chantent, matin]      oiseaux chantent matin  \n",
      "2        [chien, court, jardin]          chien court jardin  \n",
      "3    [mangeon, pomm, délicieus]      mangeon pomm délicieus  \n",
      "4         [mang, orang, fraîch]           mang orang fraîch  \n"
     ]
    }
   ],
   "source": [
    "# Appliquer la fonction remove_stopwords à la colonne 'tokens'\n",
    "df['tokens_cleaned'] = df['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b235",
   "metadata": {},
   "source": [
    "### 8. Appliquer la lemmatisation et stremming sur le corpus sans stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0948914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# def lemmatize_and_stem(text):\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     stemmer = PorterStemmer()\n",
    "    \n",
    "#     tokens = word_tokenize(text)\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "#     stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
    "    \n",
    "#     return ' '.join(stemmed_tokens)\n",
    "\n",
    "# # Supposons que 't_s_p' est déjà ajoutée à votre DataFrame df\n",
    "# df['lemmatized_and_stemmed_text'] = df['tokens'].apply(lemmatize_and_stem)\n",
    "\n",
    "# # Afficher le DataFrame mis à jour\n",
    "# print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cd97ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                   tokens_cleaned  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                 lemmatized_tokens              lemmatized_text  \\\n",
      "0              [chat, dort, tapis]              chat dort tapis   \n",
      "1       [Oiseaux, Chantent, Matin]       Oiseaux Chantent Matin   \n",
      "2           [chien, court, jardin]           chien court jardin   \n",
      "3  [Mangeons, pommes, délicieuses]  Mangeons pommes délicieuses   \n",
      "4         [mange, orange, fraîche]         mange orange fraîche   \n",
      "\n",
      "  lemmatized_and_stemmed_tokens lemmatized_and_stemmed_text  \n",
      "0            [chat, dort, tapi]              chat dort tapi  \n",
      "1    [oiseaux, chantent, matin]      oiseaux chantent matin  \n",
      "2        [chien, court, jardin]          chien court jardin  \n",
      "3    [mangeon, pomm, délicieus]      mangeon pomm délicieus  \n",
      "4         [mang, orang, fraîch]           mang orang fraîch  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importer la bibliothèque nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Définir la fonction de lemmatisation\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Appliquer la lemmatisation à la colonne 'tokens_cleaned'\n",
    "df['lemmatized_tokens'] = df['tokens_cleaned'].apply(lemmatize_tokens)\n",
    "\n",
    "# Convertir les listes de tokens en chaînes de caractères\n",
    "df['lemmatized_text'] = df['lemmatized_tokens'].apply(' '.join)\n",
    "\n",
    "# Afficher le DataFrame mis à jour\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441d72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cab3e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                   tokens_cleaned  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                 lemmatized_tokens              lemmatized_text  \\\n",
      "0              [chat, dort, tapis]              chat dort tapis   \n",
      "1       [Oiseaux, Chantent, Matin]       Oiseaux Chantent Matin   \n",
      "2           [chien, court, jardin]           chien court jardin   \n",
      "3  [Mangeons, pommes, délicieuses]  Mangeons pommes délicieuses   \n",
      "4         [mange, orange, fraîche]         mange orange fraîche   \n",
      "\n",
      "  lemmatized_and_stemmed_tokens lemmatized_and_stemmed_text  \n",
      "0            [chat, dort, tapi]              chat dort tapi  \n",
      "1    [oiseaux, chantent, matin]      oiseaux chantent matin  \n",
      "2        [chien, court, jardin]          chien court jardin  \n",
      "3    [mangeon, pomm, délicieus]      mangeon pomm délicieus  \n",
      "4         [mang, orang, fraîch]           mang orang fraîch  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importer la bibliothèque nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Définir la fonction de stemming\n",
    "def stem_tokens(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Appliquer le stemming à la colonne 'tokens_cleaned'\n",
    "df['lemmatized_and_stemmed_tokens'] = df['lemmatized_tokens'].apply(stem_tokens)\n",
    "# Convertir les listes de tokens en chaînes de caractères\n",
    "df['lemmatized_and_stemmed_text'] = df['lemmatized_and_stemmed_tokens'].apply(' '.join)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a3981",
   "metadata": {},
   "source": [
    "## Partie 2 : CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb22ab7",
   "metadata": {},
   "source": [
    "### 9. Initialiser et ajuster le CountVectorizer à votre corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07385b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a74318f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chantent' 'chat' 'chien' 'court' 'dort' 'délicieus' 'fraîch' 'jardin'\n",
      " 'mang' 'mangeon' 'matin' 'oiseaux' 'orang' 'pomm' 'tapi']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialiser le CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Ajuster le vectorizer à votre corpus\n",
    "X = vectorizer.fit_transform(df['lemmatized_and_stemmed_text'])\n",
    "\n",
    "# Afficher les mots du vocabulaire\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f15a96c",
   "metadata": {},
   "source": [
    "### 10. Transformer le corpus en une matrice de comptage de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c045750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser le vectorizer précédemment initialisé et ajusté\n",
    "X = vectorizer.transform(df['lemmatized_and_stemmed_text'])\n",
    "\n",
    "# Convertir la matrice en un DataFrame pour une meilleure visualisation\n",
    "import pandas as pd\n",
    "count_matrix = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c350ce",
   "metadata": {},
   "source": [
    "### 11. Explorer la matrice résultante pour comprendre comment les tokens sont représentés en vecteurs binaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6188bb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chantent  chat  chien  court  dort  délicieus  fraîch  jardin  mang  \\\n",
      "0         0     1      0      0     1          0       0       0     0   \n",
      "1         1     0      0      0     0          0       0       0     0   \n",
      "2         0     0      1      1     0          0       0       1     0   \n",
      "3         0     0      0      0     0          1       0       0     0   \n",
      "4         0     0      0      0     0          0       1       0     1   \n",
      "\n",
      "   mangeon  matin  oiseaux  orang  pomm  tapi  \n",
      "0        0      0        0      0     0     1  \n",
      "1        0      1        1      0     0     0  \n",
      "2        0      0        0      0     0     0  \n",
      "3        1      0        0      0     1     0  \n",
      "4        0      0        0      1     0     0  \n"
     ]
    }
   ],
   "source": [
    "# Afficher la matrice de comptage de tokens\n",
    "print(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85be9408",
   "metadata": {},
   "source": [
    "## Partie 3 : TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6becbc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialiser le TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajuster le TfidfVectorizer à votre corpus\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['lemmatized_and_stemmed_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633ad4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chantent     chat    chien    court     dort  délicieus   fraîch   jardin  \\\n",
      "0   0.00000  0.57735  0.00000  0.00000  0.57735    0.00000  0.00000  0.00000   \n",
      "1   0.57735  0.00000  0.00000  0.00000  0.00000    0.00000  0.00000  0.00000   \n",
      "2   0.00000  0.00000  0.57735  0.57735  0.00000    0.00000  0.00000  0.57735   \n",
      "3   0.00000  0.00000  0.00000  0.00000  0.00000    0.57735  0.00000  0.00000   \n",
      "4   0.00000  0.00000  0.00000  0.00000  0.00000    0.00000  0.57735  0.00000   \n",
      "\n",
      "      mang  mangeon    matin  oiseaux    orang     pomm     tapi  \n",
      "0  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.57735  \n",
      "1  0.00000  0.00000  0.57735  0.57735  0.00000  0.00000  0.00000  \n",
      "2  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000  \n",
      "3  0.00000  0.57735  0.00000  0.00000  0.00000  0.57735  0.00000  \n",
      "4  0.57735  0.00000  0.00000  0.00000  0.57735  0.00000  0.00000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir la matrice TF-IDF en un DataFrame pour une meilleure visualisation\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Afficher le DataFrame\n",
    "print(tfidf_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f97654",
   "metadata": {},
   "source": [
    " ### 3. Explorer la matrice résultante pour comprendre comment les tokens sont représentés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a385a954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poids TF-IDF pour le terme 'chat':\n",
      "0    0.57735\n",
      "1    0.00000\n",
      "2    0.00000\n",
      "3    0.00000\n",
      "4    0.00000\n",
      "Name: chat, dtype: float64\n",
      "\n",
      "Poids TF-IDF pour le terme 'chien':\n",
      "0    0.00000\n",
      "1    0.00000\n",
      "2    0.57735\n",
      "3    0.00000\n",
      "4    0.00000\n",
      "Name: chien, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Afficher les poids TF-IDF pour les termes \"chat\" et \"chien\"\n",
    "print(\"Poids TF-IDF pour le terme 'chat':\")\n",
    "print(tfidf_df['chat'])\n",
    "\n",
    "print(\"\\nPoids TF-IDF pour le terme 'chien':\")\n",
    "print(tfidf_df['chien'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b6831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9995f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité de cosinus entre 'chat' et 'chien': 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sélectionner les vecteurs TF-IDF pour les termes \"chat\" et \"chien\"\n",
    "vector_chat = tfidf_df[['chat']].values\n",
    "vector_chien = tfidf_df[['chien']].values\n",
    "\n",
    "# Calculer la similarité de cosinus\n",
    "similarity_chat_chien = cosine_similarity(vector_chat, vector_chien)\n",
    "\n",
    "# Afficher la similarité\n",
    "print(f\"Similarité de cosinus entre 'chat' et 'chien': {similarity_chat_chien[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63408c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
